if (is(obj, "try-error")) return(NA) else return(obj$estimate)
}
row.def<-list(1:8,9:16,17:24,25:32,33:40,41:48,49:54,55:60,61:66,67:72,73:78,79:84)
head(aaoa)
multi.test3=function(input=aaoa,row.def=row.def){
n.col<-ncol(input)
head(input)
pval<-c()
fc<-c()
all.pv<-data.frame()
all.fc<-data.frame()
#input[,i]
for(i in 1:n.col){
pval<-c()
fc<-c()
for(j in 2:6){
pval[j-1]<-my.t.test.pv(input[row.def[[1]],i],input[row.def[[j]],i])
fc[j-1]<-log(my.t.test.est(input[row.def[[1]],i],input[row.def[[j]],i])[2]/my.t.test.est(input[row.def[[1]],i],input[row.def[[j]],i])[1],base=2)
}
for(j in 8:12){
pval[j-2]<-my.t.test.pv(input[row.def[[7]],i],input[row.def[[j]],i])
fc[j-2]<-log(my.t.test.est(input[row.def[[7]],i],input[row.def[[j]],i])[2]/my.t.test.est(input[row.def[[7]],i],input[row.def[[j]],i])[1],base=2)
}
all.pv<-rbind(all.pv,pval)
rownames(all.pv)[i]<-paste(colnames(input)[i])
all.fc<-rbind(all.fc,fc)
rownames(all.fc)[i]<-paste(colnames(input)[i])
}
colnames(all.fc)<-paste(1:10)
colnames(all.pv)<-paste(1:10)
all<-list(all.fc=all.fc,all.pv=all.pv)
return(all)
}
all<-multi.test3(input=aaoa,row.def=row.def)
warnings()
library(qvalue)
all.fc<-all$all.fc
all.pv<-all$all.pv
all.qmat.aaoa<-matrix(qvalue(p=unlist(all$all.pv))$qvalues,byrow = FALSE,ncol=ncol(all.pv))
hist(qvalue(p=unlist(all$all.pv)))
colnames(all.qmat.aaoa)<-colnames(all$all.pv)
colnames(all.fc)<-paste(c(rep("2w",times=5),rep("10w",times=5)),gns[2:6])
row.names(all.qmat.aaoa)<-row.names(all$all.pv)
head(all.qmat.aaoa)
### those with 0.01
q01<-all.fc
q05<-all.fc
#all.fc[all.qmat.aaoa>0.01]<-NA
q05[all.qmat.aaoa>0]<-""
q05[all.qmat.aaoa<0.05]<-"*"
q05[all.qmat.aaoa<0.01]<-"**"
all.fc[all.qmat.aaoa>0.05]<-NA
#all.qmat.aaoa
#all.fc[abs(all.fc)<0.58]<-NA
### heatmap of acyl coA changes
?heatmap.2
library(ggplot2)
library(gplots)
my_palette<-colorRampPalette(c("blue","white","red"))(n=50)
heatmap.2(as.matrix(all.fc),trace="none",scale="none",Colv = F,Rowv=F,
na.col="grey",dendrogram = "none",col=my_palette,tracecol = "black",
key.title = "",
margins = c(10,10),
cellnote=q05,  notecol="black")
heatmap.2(as.matrix(all.fc),trace="none",scale="none",Colv = F,Rowv=F,
na.col="grey",dendrogram = "none",col=my_palette,tracecol = "black",
key.title = "",
margins = c(10,10), sep=5,
cellnote=q05,  notecol="black")
heatmap.2(as.matrix(all.fc),trace="none",scale="none",Colv = F,Rowv=F,
na.col="grey",dendrogram = "none",col=my_palette,tracecol = "black",
key.title = "",
margins = c(10,10), colsep = 5,
cellnote=q05,  notecol="black")
list.files("~/R24/metabolites/")
aaoa<-read.delim(file="~/R24/metabolites/AC.txt",header = T, row.names = 1,stringsAsFactors = F)
head(aaoa)
all<-multi.test3(input=aaoa,row.def=row.def)
warnings()
library(qvalue)
all.fc<-all$all.fc
all.pv<-all$all.pv
all.qmat.aaoa<-matrix(qvalue(p=unlist(all$all.pv))$qvalues,byrow = FALSE,ncol=ncol(all.pv))
hist(qvalue(p=unlist(all$all.pv)))
colnames(all.qmat.aaoa)<-colnames(all$all.pv)
colnames(all.fc)<-paste(c(rep("2w",times=5),rep("10w",times=5)),gns[2:6])
row.names(all.qmat.aaoa)<-row.names(all$all.pv)
head(all.qmat.aaoa)
q01<-all.fc
q05<-all.fc
q05[all.qmat.aaoa>0]<-""
q05[all.qmat.aaoa<0.05]<-"*"
q05[all.qmat.aaoa<0.01]<-"**"
all.fc[all.qmat.aaoa>0.05]<-NA
?heatmap.2
library(ggplot2)
library(gplots)
my_palette<-colorRampPalette(c("blue","white","red"))(n=50)
heatmap.2(as.matrix(all.fc),trace="none",scale="none",Colv = F,Rowv=F,
na.col="grey",dendrogram = "none",col=my_palette,tracecol = "black",
key.title = "",
margins = c(10,10), colsep = 5,
cellnote=q05,  notecol="black")
all.fc
my_palette<-colorRampPalette(c("blue","white","red"))(n=50)
heatmap.2(as.matrix(all.fc),trace="none",scale="none",Colv = F,Rowv=F,
na.col="grey",dendrogram = "none",col=my_palette,tracecol = "black",
key.title = "",
margins = c(10,10), colsep = 5,
cellnote=q05,  notecol="black")
aaoa
aaoa=aaoa+0.0000001
aaoa
all<-multi.test3(input=aaoa,row.def=row.def)
warnings()
library(qvalue)
all.fc<-all$all.fc
all.pv<-all$all.pv
all.qmat.aaoa<-matrix(qvalue(p=unlist(all$all.pv))$qvalues,byrow = FALSE,ncol=ncol(all.pv))
hist(qvalue(p=unlist(all$all.pv)))
colnames(all.qmat.aaoa)<-colnames(all$all.pv)
colnames(all.fc)<-paste(c(rep("2w",times=5),rep("10w",times=5)),gns[2:6])
row.names(all.qmat.aaoa)<-row.names(all$all.pv)
head(all.qmat.aaoa)
q01<-all.fc
q05<-all.fc
q05[all.qmat.aaoa>0]<-""
q05[all.qmat.aaoa<0.05]<-"*"
q05[all.qmat.aaoa<0.01]<-"**"
all.fc[all.qmat.aaoa>0.05]<-NA
?heatmap.2
library(ggplot2)
library(gplots)
my_palette<-colorRampPalette(c("blue","white","red"))(n=50)
heatmap.2(as.matrix(all.fc),trace="none",scale="none",Colv = F,Rowv=F,
na.col="grey",dendrogram = "none",col=my_palette,tracecol = "black",
key.title = "",
margins = c(10,10), colsep = 5,
cellnote=q05,  notecol="black")
44*0.7
44*1.47
44*2.1
92.1+6.1
92.4/48
341.5/491.5
89+162
89/251
2/288
0.006944*504
0.33/0.2
260*0.002
14*255
68.8/773.85
0.7*0.42
log(base=2, 3.413)
2^3.413
7*120
11*120
6*183
0.76*30
log(4,base=2)
log(16,base=2)
2^6
2^3
2^9
2000/2.5
40*30*200
1.04^5
25*(1.04^5)
install.packages("tensorflow")
install.packages("keras")
install.packages('devtools')
#installing keras
devtools::install_github("rstudio/keras")
library(keras)
install_tensorflow()
devtools::install_github("rstudio/keras")
library(keras)
install_tensorflow()
install_tensorflow(GPU=T)
?dataset_cifar10 #to see the help file for details of dataset
cifar<-dataset_cifar10()
dataset_cifar10()
library(reticulate)
install_tensorflow()
cifar<-dataset_cifar10()
?Rtools
devtools::install_github("rstudio/keras")
library(reticulate)
library(keras)
cifar<-dataset_cifar10()
cifar<-dataset_cifar10()
cifar<-dataset_cifar10()
devtools::install_github("rstudio/keras")
library(keras)
library(reticulate)
install_tensorflow()
cifar<-dataset_cifar10()
cifar<-dataset_cifar10()
library(keras)
library(reticulate)
cifar<-dataset_cifar10()
library(keras)
library(reticulate)
install_tensorflow()
install_tensorflow(GPU=T)
?dataset_cifar10 #to see the help file for details of dataset
cifar<-dataset_cifar10()
devtools::install_github("rstudio/keras")
install_tensorflow()
??`keras-package`
install_keras()
install_keras()
install_keras()
install_keras()
install_keras()
library(keras)
install_keras()
install_keras()
setwd("~/GitHub/deeplearning/")
list.files()
lfq1<-read.delim("lfq1.txt",stringsAsFactors = F,header = T)
head(lfq1)
lfq1[3,5]
lfq2<-read.delim("lfq2.txt",stringsAsFactors = F,header = T)
head(lfq2
)
head(lfq2,100)
lfq2[1:10,1:10]
list.files()
lfq1sd<-read.delim("lfq1sd.txt",stringsAsFactors = F,header = T)
lfq2sd<-read.delim("lfq2sd.txt",stringsAsFactors = F,header = T) ### missing values become NA
?rnorm
rnorm(mean=mean,sd=sd)
rnorm(n=3,mean=mean,sd=sd)
mean=lfq1[3,1]
sd=lfq1sd[3,1]
rnorm(n=3,mean=mean,sd=sd)
rnorm(n=3,mean=mean,sd=sd)
mean
sd=lfq1sd[1,3]
mean=lfq1[1,3]
rnorm(n=3,mean=mean,sd=sd)
print(mean)
print(sd)
dcomp=function(mean=lfq1[1,3],sd=lfq1sd[1,3]){
print(mean)
print(sd)
rnorm(n=3,mean=mean,sd=sd)
}
dcomp(mean=lfq[,3],sd=lfq1sd[,3])
dcomp(n=3,mean=lfq1[,3],sd=lfq1sd[,3])
dcomp(mean=lfq1[,3],sd=lfq1sd[,3])
sd=lfq1sd[,3]
mean=lfq1[,3]
sd
out<-rnorm(n=3,mean=mean,sd=sd)
out
out<-rnorm(n=3,mean=mean,sd=sd)
out
sd
apply(dcomp, mean=mean, sd=sd)
apply(FUN=dcomp, mean=mean, sd=sd)
apply(FUN=dcomp, X= lfq[3],mean=mean, sd=sd)
apply(FUN=dcomp, X= lfq1[,3],mean=mean, sd=sd)
lapply(FUN=dcomp, X= lfq1[,3],mean=mean, sd=sd)
lapply(FUN=dcomp, mean=mean, sd=sd)
ncol(lfq1)
in.sds<-lfq1sd
in.means<-lfq1
ncol(in.means)
cbind(in.means,in.means1[3:ncol(in.means)],in.means1[3:ncol(in.means)])
cbind(in.means,in.means[,3:ncol(in.means)],in.means[,3:ncol(in.means)])
dc.vals<-cbind(in.means,in.means[,3:ncol(in.means)],in.means[,3:ncol(in.means)])
ncol(dc.vals)
colnames(in.means)
is.na(dc.vals)
dc.vals[is.na(dc.vals)==FALSE]<-NA
head(dc.vals)
colnames(in.means)
colnames(in.means)[3:176]
colnames(in.means)
paste(c(1:3), colnames(in.means)[3:176])
apply(paste, c(1:3), colnames(in.means)[3:176])
lapply(paste, c(1:3), colnames(in.means)[3:176])
lapply(FUN=paste, c(1:3), colnames(in.means)[3:176])
tapply(FUN=paste, c(1:3), colnames(in.means)[3:176])
vapply(FUN=paste, c(1:3), colnames(in.means)[3:176])
lapply(FUN=paste, c(1:3), colnames(in.means)[3:176])
lapply(FUN=paste, colnames(in.means)[3:176],c(1:3), sep="")
colnames<-unlist(lapply(FUN=paste, colnames(in.means)[3:176],c(1:3), sep=""))
new.colnames<-unlist(lapply(FUN=paste, colnames(in.means)[3:176],c(1:3), sep=""))
new.colnames
new.colnames<-unlist(lapply(FUN=paste, colnames(in.means)[3:176],c(1:3), sep=".n"))
new.colnames
new.colnames<-c(colnames(in.means)[1:2],unlist(lapply(FUN=paste, colnames(in.means)[3:176],c(1:3), sep=".n")))
new.colnames
colnames(dc.vals)
colnames(dc.vals)<-new.colnames
colnames(dc.vals)
dc.vals[1:2,]
dc.vals[1:2,]<-in.means[1:2,]
dc.vals[1:2,]
dc.vals[,1:2]<-in.means[,1:2]
dc.vals[,1:2]
i
i=1
3*(i-1)+3
3*(i-1)+c(3:5)
i=2
3*(i-1)+c(3:5)
j=1
3*(i-3)+c(3:5)
i=3
3*(i-3)+c(3:5)
i=2
3*(i-3)+c(3:5)
i=4
3*(i-3)+c(3:5)
for(i in 3:ncol(in.means)){
for(j in 1:nrow(in.means)){
fill.cols<-3*(i-3)+c(3:5)
dc.vals[j,fill.cols]<-dcomp(mean=in.means[j,i], sd=in.sds[j,i])
}
}
warnings
warnings()
head(dc.vals)
write.table(file="decomposedtest1.txt",dc.vals,head=T,quote=F, sep="\t")
write.table(file="decomposedtest1.txt",dc.vals,quote=F, sep="\t")
dcomp=function(mean=lfq1[1,3],sd=lfq1sd[1,3]){
#print(mean)
#print(sd)
out<-rnorm(n=3,mean=mean,sd=sd)
out
}
in.means<-lfq2
in.sds<-lfq2sd
in.means<-lfq2
in.sds<-lfq2sd
for(i in 3:ncol(in.means)){
for(j in 1:nrow(in.means)){
fill.cols<-3*(i-3)+c(3:5)
dc.vals[j,fill.cols]<-dcomp(mean=in.means[j,i], sd=in.sds[j,i])
}
print(i)
}
warnings9
warnings()
write.table(file="decomposedtest2.txt",dc.vals,quote=F, sep="\t",row.names = F)
new.colnames2<-paste("F",new.colnames,sep=".")
new.colnames2
colnames(dc.vals)<-new.colnames2
write.table(file="decomposedtest2.txt",dc.vals,quote=F, sep="\t",row.names = F)
lfq.all<-read.delim("decomposed_all.txt",stringsAsFactors = F,header = T) ### missing values become NA
install_keras()
?dataset_cifar10 #to see the help file for details of dataset
cifar<-dataset_cifar10()
train_x<-cifar$train$x/255
train_y<-to_categorical(cifar$train$y,num_classes = 10)
train_y
cifar$train$y
test_x<-cifar$test$x/255
test_y<-to_categorical(cifar$test$y,num_classes=10)
dim(train_x)
cifar$train$x
cat("No of training samples\t",dim(train_x)[[1]],"\tNo of test samples\t",dim(test_x)[[1]])
model<-keras_model_sequential()
model %>%
#defining a 2-D convolution layer
layer_conv_2d(filter=32,kernel_size=c(3,3),padding="same",                input_shape=c(32,32,3) ) %>%
layer_activation("relu") %>%
#another 2-D convolution layer
layer_conv_2d(filter=32 ,kernel_size=c(3,3))  %>%  layer_activation("relu") %>%
#Defining a Pooling layer which reduces the dimentions of the #features map and reduces the computational complexity of the model
layer_max_pooling_2d(pool_size=c(2,2)) %>%
#dropout layer to avoid overfitting
layer_dropout(0.25) %>%
layer_conv_2d(filter=32 , kernel_size=c(3,3),padding="same") %>% layer_activation("relu") %>%  layer_conv_2d(filter=32,kernel_size=c(3,3) ) %>%  layer_activation("relu") %>%
layer_max_pooling_2d(pool_size=c(2,2)) %>%
layer_dropout(0.25) %>%
#flatten the input
layer_flatten() %>%
layer_dense(512) %>%
layer_activation("relu") %>%
layer_dropout(0.5) %>%
#output layer-10 classes-10 units
layer_dense(10) %>%
#applying softmax nonlinear activation function to the output layer #to calculate cross-entropy
layer_activation("softmax")
opt<-optimizer_adam( lr= 0.0001 , decay = 1e-6 )
model %>%
compile(loss="categorical_crossentropy",
optimizer=opt,metrics = "accuracy")
summary(model)
data_augmentation <- TRUE
if(!data_augmentation) {
model %>% fit( train_x,train_y ,batch_size=32,
epochs=80,validation_data = list(test_x, test_y),
shuffle=TRUE)
}
else {
#Generating images
gen_images <- image_data_generator(featurewise_center = TRUE,
featurewise_std_normalization = TRUE,
rotation_range = 20,
width_shift_range = 0.30,
height_shift_range = 0.30,
horizontal_flip = TRUE  )
#Fit image data generator internal statistics to some sample data
gen_images %>% fit_image_data_generator(train_x)
#Generates batches of augmented/normalized data from image data and #labels to visually see the generated images by the Model
model %>% fit_generator(
flow_images_from_data(train_x, train_y,gen_images,
batch_size=32,save_to_dir="F:/PROJECTS/CNNcifarimages/"),
steps_per_epoch=as.integer(50000/32),epochs = 5,
validation_data = list(test_x, test_y) )
}
if(data_augmentation) {
#Generating images
gen_images <- image_data_generator(featurewise_center = TRUE,
featurewise_std_normalization = TRUE,
rotation_range = 20,
width_shift_range = 0.30,
height_shift_range = 0.30,
horizontal_flip = TRUE  )
#Fit image data generator internal statistics to some sample data
gen_images %>% fit_image_data_generator(train_x)
#Generates batches of augmented/normalized data from image data and #labels to visually see the generated images by the Model
model %>% fit_generator(
flow_images_from_data(train_x, train_y,gen_images,
batch_size=32,save_to_dir="F:/PROJECTS/CNNcifarimages/"),
steps_per_epoch=as.integer(50000/32),epochs = 5,
validation_data = list(test_x, test_y) )
}
if(data_augmentation) {
#Generating images
gen_images <- image_data_generator(featurewise_center = TRUE,
featurewise_std_normalization = TRUE,
rotation_range = 20,
width_shift_range = 0.30,
height_shift_range = 0.30,
horizontal_flip = TRUE  )
#Fit image data generator internal statistics to some sample data
gen_images %>% fit_image_data_generator(train_x)
#Generates batches of augmented/normalized data from image data and #labels to visually see the generated images by the Model
model %>% fit_generator(
flow_images_from_data(train_x, train_y,gen_images,
batch_size=32,save_to_dir="D:/PROJECTS/CNNcifarimages/"),
steps_per_epoch=as.integer(50000/32),epochs = 5,
validation_data = list(test_x, test_y) )
}
if(data_augmentation) {
#Generating images
gen_images <- image_data_generator(featurewise_center = TRUE,
featurewise_std_normalization = TRUE,
rotation_range = 20,
width_shift_range = 0.30,
height_shift_range = 0.30,
horizontal_flip = TRUE  )
#Fit image data generator internal statistics to some sample data
gen_images %>% fit_image_data_generator(train_x)
#Generates batches of augmented/normalized data from image data and #labels to visually see the generated images by the Model
model %>% fit_generator(
flow_images_from_data(train_x, train_y,gen_images,
batch_size=32,save_to_dir="D:/PROJECTS/cnncifarimages/"),
steps_per_epoch=as.integer(50000/32),epochs = 5,
validation_data = list(test_x, test_y) )
}
if(data_augmentation) {
#Generating images
gen_images <- image_data_generator(featurewise_center = TRUE,
featurewise_std_normalization = TRUE,
rotation_range = 20,
width_shift_range = 0.30,
height_shift_range = 0.30,
horizontal_flip = TRUE  )
#Fit image data generator internal statistics to some sample data
gen_images %>% fit_image_data_generator(train_x)
#Generates batches of augmented/normalized data from image data and #labels to visually see the generated images by the Model
model %>% fit_generator(
flow_images_from_data(train_x, train_y,gen_images,
batch_size=32,save_to_dir="D:/PROJECTS/cnncifarimages/"),
steps_per_epoch=as.integer(50000/32),epochs = 100,
validation_data = list(test_x, test_y) )
}
opt<-optimizer_adam( lr= 0.001 , decay = 1e-6 )
if(data_augmentation) {
#Generating images
gen_images <- image_data_generator(featurewise_center = TRUE,
featurewise_std_normalization = TRUE,
rotation_range = 20,
width_shift_range = 0.30,
height_shift_range = 0.30,
horizontal_flip = TRUE  )
#Fit image data generator internal statistics to some sample data
gen_images %>% fit_image_data_generator(train_x)
#Generates batches of augmented/normalized data from image data and #labels to visually see the generated images by the Model
model %>% fit_generator(
flow_images_from_data(train_x, train_y,gen_images,
batch_size=32,save_to_dir="D:/PROJECTS/cnncifarimages/"),
steps_per_epoch=as.integer(50000/32),epochs = 5,
validation_data = list(test_x, test_y) )
}
1000/2
1000/24
40*24
4000/960
70/60
4000/1.1666
10000/48
15000/48
